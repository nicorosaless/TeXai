# LaTeX AI Backend

Backend API para el asistente de IA especializado en documentos LaTeX. Proporciona funcionalidades similares a Cursor AI pero especÃ­ficamente diseÃ±adas para trabajar con LaTeX.

## ğŸš€ CaracterÃ­sticas

- **Chat Inteligente**: ConversaciÃ³n contextual sobre documentos LaTeX
- **AnÃ¡lisis de Documentos**: DetecciÃ³n de errores, advertencias y sugerencias
- **Mejora AutomÃ¡tica**: OptimizaciÃ³n de escritura, formato, ecuaciones y estructura
- **Ollama Integration**: Usa modelos de IA locales ejecutados con Ollama
- **Listado de Modelos**: Endpoint para listar modelos disponibles usando `ollama list`
- **Streaming**: Respuestas en tiempo real mediante Server-Sent Events
- **API RESTful**: Endpoints bien documentados con FastAPI

## ğŸ“‹ Requisitos

- Python 3.10 o superior
- pip para gestiÃ³n de dependencias
- **Ollama instalado** (https://ollama.ai)
- Al menos un modelo de Ollama descargado (ej: `ollama pull llama2`)

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio** (si aÃºn no lo has hecho):
```bash
cd /Users/testnico/Documents/GitHub/TeXai
```

2. **Crear entorno virtual**:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

4. **Instalar y configurar Ollama**:
```bash
# Instalar Ollama desde https://ollama.ai
# O usando Homebrew (macOS):
brew install ollama

# Iniciar Ollama (si no estÃ¡ corriendo como servicio)
ollama serve

# Descargar un modelo (ejemplo con llama2)
ollama pull llama2
```

5. **Configurar variables de entorno**:
```bash
cp .env.example .env
# Editar .env si necesitas cambiar la URL o modelo por defecto
```

## ğŸƒ EjecuciÃ³n

### Modo Desarrollo
```bash
python main.py
```

O usando uvicorn directamente:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

El servidor estarÃ¡ disponible en: `http://localhost:8000`

### DocumentaciÃ³n de la API
Una vez ejecutando, puedes acceder a:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“¡ Endpoints Principales

### 1. Chat con el Asistente
```http
POST /api/v1/chat
Content-Type: application/json

{
  "message": "Mejora la introducciÃ³n de mi documento",
  "latex_content": "\\documentclass{article}...",
  "conversation_history": [],
  "stream": false
}
```

### 2. AnÃ¡lisis de Documento
```http
POST /api/v1/analyze
Content-Type: application/json

{
  "latex_content": "\\documentclass{article}..."
}
```

### 3. Mejora de Documento
```http
POST /api/v1/improve
Content-Type: application/json

{
  "latex_content": "\\documentclass{article}...",
  "improvement_type": "writing",
  "focus_areas": ["abstract", "introduction"]
}
```

### 4. Listar Modelos Disponibles
```http
GET /api/v1/models
```

### 5. Obtener Modelo Actual
```http
GET /api/v1/models/current
```

### 6. Health Check
```http
GET /health
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Default |
|----------|-------------|---------|
| `HOST` | Host del servidor | `0.0.0.0` |
| `PORT` | Puerto del servidor | `8000` |
| `DEBUG` | Modo debug | `False` |
| `OLLAMA_BASE_URL` | URL base de Ollama | `http://localhost:11434` |
| `OLLAMA_MODEL` | Modelo de Ollama por defecto | `llama2` |
| `MAX_TOKENS` | MÃ¡ximo de tokens en respuesta | `4000` |
| `TEMPERATURE` | Temperatura del modelo | `0.7` |
| `MAX_LATEX_LENGTH` | Longitud mÃ¡xima del documento | `50000` |

## ğŸ—ï¸ Estructura del Proyecto

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # ConfiguraciÃ³n de la aplicaciÃ³n
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py          # Modelos Pydantic
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py            # Endpoints de chat
â”‚   â”‚   â”œâ”€â”€ analyze.py         # Endpoints de anÃ¡lisis
â”‚   â”‚   â””â”€â”€ improve.py         # Endpoints de mejora
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ ai_service.py      # Servicio de IA
â”œâ”€â”€ main.py                     # AplicaciÃ³n principal
â”œâ”€â”€ requirements.txt            # Dependencias
â”œâ”€â”€ .env.example                # Ejemplo de variables de entorno
â””â”€â”€ README.md                   # Este archivo
```

## ğŸ§ª Testing

```bash
# Instalar dependencias de desarrollo
pip install pytest pytest-asyncio

# Ejecutar tests
pytest
```

## ğŸ”Œ IntegraciÃ³n con Frontend

El frontend debe hacer requests a:
- Base URL: `http://localhost:8000`
- Endpoints: `/api/v1/chat`, `/api/v1/analyze`, `/api/v1/improve`

Ejemplo de integraciÃ³n en el frontend:
```typescript
const response = await fetch('http://localhost:8000/api/v1/chat', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    message: userMessage,
    latex_content: latexContent,
    conversation_history: history,
    stream: false
  })
});

const data = await response.json();
```

## ğŸš€ Despliegue

### Docker (prÃ³ximamente)
```bash
docker build -t latex-ai-backend .
docker run -p 8000:8000 --env-file .env latex-ai-backend
```

### ProducciÃ³n
Para producciÃ³n, usar un servidor ASGI como:
- **Gunicorn + Uvicorn workers**
- **Docker + Nginx**
- **Cloud platforms** (Railway, Render, Fly.io)

## ğŸ“ Notas

- **Ollama debe estar ejecutÃ¡ndose** antes de iniciar el backend
- Los modelos se ejecutan localmente, no requiere conexiÃ³n a internet (excepto para descargar modelos)
- Para mejores resultados, se recomienda usar modelos grandes como `llama2`, `mistral`, `codellama`, etc.
- Puedes listar modelos disponibles con `ollama list` o usando el endpoint `/api/v1/models`
- Para cambiar el modelo, edita `OLLAMA_MODEL` en el archivo `.env` o usa el endpoint de modelos

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es privado.

## ğŸ†˜ Soporte

Para problemas o preguntas, abre un issue en el repositorio.


# Integraci√≥n Frontend-Backend

## ‚úÖ Integraci√≥n Completada

El frontend y backend est√°n completamente conectados. El modelo local de Ollama puede ahora modificar c√≥digo LaTeX directamente.

## üöÄ C√≥mo Usar

### 1. Iniciar el Backend

```bash
cd backend
./run.sh
```

O manualmente:
```bash
cd backend
source venv/bin/activate
python main.py
```

El backend estar√° disponible en `http://localhost:8000`

### 2. Iniciar el Frontend

```bash
cd frontend
npm install  # Solo la primera vez
npm run dev
```

El frontend estar√° disponible en `http://localhost:8080`

### 3. Usar el Asistente

1. **Chat Normal**: Escribe preguntas sobre tu documento LaTeX
2. **Mejoras Autom√°ticas**: 
   - Haz clic en "Improve writing" para mejorar la escritura
   - Haz clic en "Fix errors" para corregir errores autom√°ticamente
   - Escribe comandos como "mejora la introducci√≥n" o "fix errors"
3. **Aplicar Cambios**: Cuando el modelo devuelva c√≥digo modificado, aparecer√° un bot√≥n "Aplicar" para aplicar los cambios autom√°ticamente

## üîß Funcionalidades Implementadas

### Frontend (`frontend/src/`)

- ‚úÖ **Servicio API** (`services/api.ts`): Cliente para comunicarse con el backend
- ‚úÖ **ChatPanel actualizado**: 
  - Conexi√≥n real con el backend
  - Detecci√≥n autom√°tica de c√≥digo LaTeX modificado
  - Botones para aplicar cambios
  - Manejo de errores y estados de carga
- ‚úÖ **Proxy configurado**: Vite proxy para evitar problemas de CORS
- ‚úÖ **Sugerencias inteligentes**: Las sugerencias ejecutan acciones autom√°ticamente

### Backend (`backend/app/`)

- ‚úÖ **Prompt mejorado**: El sistema ahora instruye al modelo a devolver c√≥digo completo
- ‚úÖ **Extracci√≥n de c√≥digo**: Mejorada la extracci√≥n de c√≥digo LaTeX de las respuestas
- ‚úÖ **Endpoints funcionales**:
  - `/api/v1/chat` - Chat con el asistente
  - `/api/v1/analyze` - An√°lisis de documentos
  - `/api/v1/improve` - Mejora de documentos
  - `/api/v1/models` - Listar modelos disponibles

## üìù Flujo de Trabajo

1. **Usuario escribe mensaje** ‚Üí Frontend env√≠a a `/api/v1/chat`
2. **Backend procesa con Ollama** ‚Üí Modelo genera respuesta
3. **Backend extrae c√≥digo LaTeX** ‚Üí Si hay c√≥digo modificado, lo extrae
4. **Frontend recibe respuesta** ‚Üí Muestra mensaje y c√≥digo modificado
5. **Usuario aplica cambios** ‚Üí C√≥digo LaTeX se actualiza en el editor

## üéØ Ejemplos de Uso

### Mejorar Escritura
```
Usuario: "Mejora la introducci√≥n de mi documento"
‚Üí El modelo devuelve c√≥digo LaTeX mejorado
‚Üí Usuario hace clic en "Aplicar"
‚Üí El c√≥digo se actualiza autom√°ticamente
```

### Corregir Errores
```
Usuario: Clic en "Fix errors"
‚Üí Backend analiza el documento
‚Üí Encuentra errores
‚Üí Mejora el c√≥digo para corregirlos
‚Üí Usuario aplica los cambios
```

### Agregar Ecuaciones
```
Usuario: "Agrega una ecuaci√≥n para el gradiente descendente"
‚Üí Modelo genera c√≥digo LaTeX con la ecuaci√≥n
‚Üí C√≥digo se muestra en el chat
‚Üí Usuario puede aplicar o copiar manualmente
```

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

**Backend** (`.env`):
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:4b
```

**Frontend** (`.env`):
```env
VITE_API_URL=http://localhost:8000
```

## üêõ Soluci√≥n de Problemas

### El backend no responde
- Verifica que Ollama est√© corriendo: `ollama serve`
- Verifica que el modelo est√© instalado: `ollama list`
- Revisa los logs del backend

### Los cambios no se aplican
- Verifica que el modelo devuelva c√≥digo en formato ````latex`
- Revisa la consola del navegador para errores
- Verifica que el backend est√© accesible

### Errores de CORS
- El proxy de Vite deber√≠a manejar esto autom√°ticamente
- Si persisten, verifica que el backend tenga CORS configurado correctamente

## üìö Pr√≥ximos Pasos

- [ ] Agregar streaming de respuestas en tiempo real
- [ ] Implementar historial de conversaci√≥n persistente
- [ ] Agregar selecci√≥n de modelo desde el frontend
- [ ] Mejorar la UI para mostrar cambios antes de aplicar
- [ ] Agregar modo de vista previa de cambios (diff)

